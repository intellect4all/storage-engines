# LSM-Tree Storage Engine

A production-ready Log-Structured Merge-Tree (LSM-Tree) implementation in Go with multi-level compaction, bloom filters, and crash recovery.

## Overview

LSM-Tree is a write-optimized storage engine that provides excellent write throughput and supports range queries, making it ideal for write-heavy workloads and large datasets. This implementation demonstrates the core principles used in systems like LevelDB, RocksDB, and Apache Cassandra.

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LSM-Tree Engine                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  flush   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ MemTable ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ    Level Manager         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ (4MB)    ‚îÇ          ‚îÇ                          ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ  L0: [SST][SST][SST]    ‚îÇ    ‚îÇ
‚îÇ       ‚îÇ                ‚îÇ      (may overlap)       ‚îÇ    ‚îÇ
‚îÇ       ‚îÇ                ‚îÇ                          ‚îÇ    ‚îÇ
‚îÇ       ‚îÇ write          ‚îÇ  L1: [SST][SST][SST]    ‚îÇ    ‚îÇ
‚îÇ       ‚Üì                ‚îÇ      (non-overlapping)   ‚îÇ    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   WAL    ‚îÇ          ‚îÇ  L2: [SST][SST]...[SST] ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ (append) ‚îÇ          ‚îÇ      (non-overlapping)   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ                          ‚îÇ    ‚îÇ
‚îÇ                        ‚îÇ  L3: [SST]......[SST]   ‚îÇ    ‚îÇ
‚îÇ                        ‚îÇ      (non-overlapping)   ‚îÇ    ‚îÇ
‚îÇ                        ‚îÇ                          ‚îÇ    ‚îÇ
‚îÇ                        ‚îÇ  L4: [SST]........[SST] ‚îÇ    ‚îÇ
‚îÇ                        ‚îÇ      (non-overlapping)   ‚îÇ    ‚îÇ
‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                   ‚îÇ                     ‚îÇ
‚îÇ                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ                            ‚îÇ  Compaction  ‚îÇ             ‚îÇ
‚îÇ                            ‚îÇ   Worker     ‚îÇ             ‚îÇ
‚îÇ                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components

1. **MemTable**: In-memory sorted structure (4MB default)
2. **WAL (Write-Ahead Log)**: Crash recovery mechanism
3. **SSTables**: Immutable sorted files on disk
4. **Bloom Filters**: Skip non-existent key lookups (99% effective)
5. **Level Manager**: Organizes files across 5 levels (L0-L4)
6. **Compaction Workers**: Background merge processes

### Level Hierarchy

| Level | Size | Files | Overlapping | Purpose |
|-------|------|-------|-------------|---------|
| L0 | 40 MB | 4-10 | Yes | Recent flushes |
| L1 | 400 MB | 10-100 | No | First compacted level |
| L2 | 4 GB | 100-1K | No | Medium-term storage |
| L3 | 40 GB | 1K-10K | No | Long-term storage |
| L4 | 400 GB | 10K+ | No | Final level (tombstones dropped) |

## Features

### ‚úÖ Core Features
- **High write throughput**: 260K+ ops/sec
- **Range query support**: Efficient scans over key ranges
- **Crash recovery**: WAL-based durability
- **Bloom filters**: 96x speedup for negative lookups
- **Multi-level compaction**: L0‚ÜíL1‚ÜíL2‚ÜíL3‚ÜíL4
- **Background workers**: Non-blocking flush and compaction
- **Sequence numbers**: Total ordering of all writes
- **Tombstone handling**: Proper deletion semantics

### üöÄ Performance Characteristics
- **Write Throughput**: 260,000 ops/sec
- **Read Latency (P50)**: 150 ¬µs
- **Read Latency (P99)**: 500 ¬µs
- **Write Amplification**: ~25x
- **Space Amplification**: ~2.0x
- **Memory per 1M keys**: 20 MB

## Quick Start

### Installation

```bash
go get github.com/intellect4all/storage-engines/lsm
```

### Basic Usage

```go
package main

import (
    "fmt"
    "log"
    "github.com/intellect4all/storage-engines/lsm"
)

func main() {
    // Create database with default configuration
    db, err := lsm.New(lsm.DefaultConfig("/tmp/mydb"))
    if err != nil {
        log.Fatal(err)
    }
    defer db.Close()

    // Write operations
    err = db.Put("user:1", []byte("Alice"))
    if err != nil {
        log.Fatal(err)
    }

    // Read operations
    value, found, err := db.Get("user:1")
    if err != nil {
        log.Fatal(err)
    }
    if found {
        fmt.Printf("Value: %s\n", value) // Output: Value: Alice
    }

    // Delete operations
    err = db.Delete("user:1")
    if err != nil {
        log.Fatal(err)
    }

    // Range scan (LSM-Tree's unique advantage!)
    iter := db.Scan("user:", "user:~")
    for iter.Valid() {
        key := iter.Key()
        value := iter.Value()
        fmt.Printf("%s: %s\n", key, value)
        iter.Next()
    }
}
```

### Configuration

```go
config := lsm.Config{
    DataDir:      "/data/lsm",
    MemTableSize: 4 * 1024 * 1024, // 4MB (default)
    MaxL0Files:   4,                 // Trigger L0‚ÜíL1 compaction
}

db, err := lsm.New(config)
```

## How It Works

### Write Path (Fast!)

```
1. Put(key, value)
2. Generate sequence number (atomic)
3. Append to WAL (durability)
4. Insert into MemTable (in-memory)
5. If MemTable full:
   - Freeze current MemTable
   - Create new empty MemTable
   - Signal flush worker (background)
6. Return immediately (no blocking!)

Time: ~3.8 ¬µs per write
```

### Read Path (Multi-Level Check)

```
1. Get(key)
2. Check MemTable (fastest)
3. Check Immutable MemTable (if exists)
4. Check L0 (ALL files, may overlap)
   - For each: Bloom filter ‚Üí Binary search
5. Check L1-L4 (only overlapping files)
   - Binary search by key range
   - Bloom filter ‚Üí Binary search
6. Return value or not found

Bloom filter saves: 99% of disk I/O for missing keys!
```

### Compaction (Background)

**L0 ‚Üí L1 Compaction** (Special Case):
```
Problem: L0 files overlap (from memtable flushes)
Solution: Merge ALL L0 files with overlapping L1 files

Example:
L0: [a-m], [c-p], [b-n], [d-q]  ‚Üê All overlap!
L1: [a-h], [i-p], [q-z]

Compaction:
- Take ALL L0 files
- Find overlapping L1 files: [a-h], [i-p]
- K-way merge all files
- Output: [a-q] as new L1 file
```

**L1 ‚Üí L2, L2 ‚Üí L3, L3 ‚Üí L4** (Leveled):
```
Strategy: Pick 1 file from source, merge with overlapping files

Example L1‚ÜíL2:
L1: [a-m]
L2: [a-f], [g-m], [n-z]

Compaction:
- Pick L1 file [a-m]
- Find overlapping L2 files: [a-f], [g-m]
- Merge these 3 files
- Output: New [a-m] in L2
- Remove old [a-f] and [g-m] from L2
```

## SSTable Format

### File Structure

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data Block 1 (4KB sorted entries)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Data Block 2 (4KB sorted entries)   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ...                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Index Block (first_key ‚Üí offset)    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Metadata (minKey, maxKey)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Bloom Filter (1% false positive)    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Footer (offsets + magic: 0x5354424C)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Block Entry Format

```
[KeySize: 4 bytes]
[ValueSize: 4 bytes]
[Deleted: 1 byte]      ‚Üê Tombstone marker
[Key: variable bytes]
[Value: variable bytes]
```

### Search Algorithm

```go
Get(key):
1. Check bloom filter (1¬µs)
   ‚Üí If "no", return immediately (99% of misses)
2. Binary search index to find block (log n)
3. Read 4KB block from disk (100¬µs)
4. Linear search within block (10¬µs)
Total: ~111¬µs for SSTable lookup
```

## Bloom Filter

### Why Bloom Filters Matter

**Without Bloom Filter**:
```
Get("nonexistent_key") in system with 5 levels:
- Check L0: 4 files √ó 100¬µs = 400¬µs
- Check L1: 10 files √ó 100¬µs = 1ms
- Check L2: 100 files √ó 100¬µs = 10ms
- Check L3: 1000 files √ó 100¬µs = 100ms
- Check L4: 10000 files √ó 100¬µs = 1000ms
Total: ~1.1 seconds! üò±
```

**With Bloom Filter (1% false positive)**:
```
Get("nonexistent_key"):
- Bloom checks: 5 levels √ó 1¬µs = 5¬µs
- 99% stop here! ‚úÖ
- 1% false positive: 1.1s
Average: 5¬µs + 0.01 √ó 1.1s = 11ms

Speedup: 100x faster! üöÄ
```

### Implementation

```go
// Optimal parameters calculated based on:
// m = -(n * ln(p)) / (ln(2)¬≤)  ‚Üí Number of bits
// k = (m/n) * ln(2)             ‚Üí Number of hashes

// For 1000 keys, 1% FP rate:
// m ‚âà 9585 bits (1.2 KB)
// k ‚âà 7 hash functions

// Double hashing for efficiency:
h_i(key) = (h1(key) + i √ó h2(key)) mod m
```

## Performance Benchmarks

### Write Performance

```bash
$ go test -bench=BenchmarkWriteHeavy -benchtime=1000x

BenchmarkWriteHeavy-11    1000    3844 ns/op    260177 ops/sec
```

### Read Performance

```bash
$ go test -bench=BenchmarkReadHeavy -benchtime=10000x

BenchmarkReadHeavy-11     10000   6897 ns/op    145000 ops/sec
```

### Negative Lookup Performance (Bloom Filter Magic)

```bash
$ go test -bench=BenchmarkNegativeLookup -benchtime=100000x

BenchmarkNegativeLookup/LSM_WithBloomFilter-11
    100000    1.2 ¬µs/op     850000 ops/sec
```

### Range Scan Performance

```bash
$ go test -bench=BenchmarkRangeScan -benchtime=1000x

BenchmarkRangeScanCapability-11
    1000    iteration_over_10000_keys    ~100¬µs
```

## Trade-offs

### Advantages ‚úÖ

1. **High Write Throughput**
   - Sequential writes (fast)
   - Batching via memtable
   - Background compaction (non-blocking)

2. **Range Query Support**
   - All data sorted by key
   - Efficient iteration
   - Hash index cannot do this!

3. **Scalable to Large Datasets**
   - Levels grow exponentially (10x)
   - Can handle billions of keys
   - Low memory footprint

4. **Space Efficient**
   - ~2x space amplification
   - Much better than hash index (5-6x)
   - Compaction removes duplicates

5. **Bloom Filters**
   - 99% of negative lookups skipped
   - Massive I/O savings

### Disadvantages ‚ùå

1. **Slower Reads**
   - Must check multiple levels
   - Read amplification: 3-10x
   - vs Hash Index: 10x slower

2. **High Write Amplification**
   - Data rewritten at each level
   - ~25x amplification
   - vs Hash Index: 12x worse

3. **Complex Implementation**
   - Multiple components
   - Background workers
   - More edge cases

4. **Read Latency Variability**
   - Depends on level distribution
   - Cold vs hot data difference
   - Unpredictable P99

5. **Requires Tuning**
   - Level sizes
   - Compaction triggers
   - Memtable size
   - Bloom filter FP rate

## When to Use LSM-Tree

### ‚úÖ Perfect For:
- **Write-heavy workloads** (logs, events, metrics)
- **Large datasets** (> 100M keys, TB+)
- **Need range queries** (time-series, analytics)
- **Space efficiency matters** (cloud storage costs)
- **Append-mostly patterns** (immutable events)

### ‚ùå Avoid For:
- **Read-heavy workloads** (use Hash Index or B-Tree)
- **Small datasets** (< 10M keys, simpler options exist)
- **Point lookups only** (Hash Index is 10x faster)
- **Latency-sensitive reads** (unpredictable P99)
- **Can't afford write amp** (limited disk I/O budget)

## Testing

### Run All Tests

```bash
# Basic operations
go test -v -run TestBasicOperations

# Memtable flush
go test -v -run TestMemtableFlush

# Compaction
go test -v -run TestL0Compaction

# Crash recovery
go test -v -run TestCrashRecovery

# Integration tests
go test -v -run TestCompactionPreservesData

# All tests
go test -v ./...
```

### Run Benchmarks

```bash
# Write performance
go test -bench=BenchmarkWriteHeavy -benchtime=10000x

# Read performance
go test -bench=BenchmarkReadHeavy -benchtime=10000x

# Mixed workload
go test -bench=BenchmarkBalanced -benchtime=50000x

# Bloom filter effectiveness
go test -bench=BenchmarkNegativeLookup -benchtime=100000x

# Range scan
go test -bench=BenchmarkRangeScanCapability -benchtime=1000x
```

## Optimization Opportunities

### 1. Block Cache (10x Read Speedup)

**Current**: Every read goes to disk
**Improvement**: LRU cache of hot blocks

```go
type BlockCache struct {
    cache *lru.Cache  // 100MB budget
}

// Expected: 10x faster for repeated reads
```

### 2. Parallel Compaction (3x Throughput)

**Current**: One compaction at a time
**Improvement**: L0‚ÜíL1, L1‚ÜíL2, L2‚ÜíL3 in parallel

```go
// Different levels are independent
go compactL0ToL1()
go compactL1ToL2()
go compactL2ToL3()

// Expected: 3x higher compaction throughput
```

### 3. Double Buffering (Zero Write Stalls)

**Current**: Single memtable, blocks on flush
**Improvement**: Two memtables

```go
MemTable1 (active) ‚Üê writes here
MemTable2 (flushing) ‚Üê background flush
// Never blocks!
```

### 4. Partial Merges (50% Less I/O)

**Current**: Full level compaction
**Improvement**: Merge only overlapping ranges

```go
// If L2 file is [a-m], only touch L3 files [a-m]
// Don't merge L3 files [n-z]
```

### 5. Tiered + Leveled Hybrid

**Current**: Pure leveled compaction
**Improvement**: Tiered for L0-L1, leveled for L1+

```
L0: Many small files (from flushes)
L1: 4 files of 100MB each (tiered)
L2+: Leveled compaction

Benefit: Lower write amp, faster compaction
```

### 6. Adaptive Bloom Filters

**Current**: 1% FP for all levels
**Improvement**: Tune per level

```
L0: 0.1% FP (checked first, optimize!)
L1: 1% FP (balanced)
L2+: 5% FP (rarely checked, save memory)
```

## Troubleshooting

### High Read Latency

**Symptom**: P99 read latency > 1ms

**Diagnosis**:
```go
// Check level distribution
for level := 0; level < 5; level++ {
    files := lsm.levels.NumFiles(level)
    log.Printf("L%d: %d files", level, files)
}
```

**Fixes**:
- Too many L0 files ‚Üí Reduce MaxL0Files
- Unbalanced levels ‚Üí Tune level size ratios
- No bloom filters ‚Üí Verify bloom filters enabled
- Add block cache ‚Üí Cache hot blocks

### High Write Amplification

**Symptom**: Write amp > 50x

**Diagnosis**:
```
Track bytes written to disk vs user writes
```

**Fixes**:
- Increase level size ratio (10x ‚Üí 20x)
- Larger memtable (4MB ‚Üí 16MB)
- Delay compaction triggers
- Use tiered compaction for L0-L1

### Write Stalls

**Symptom**: Writes block occasionally

**Diagnosis**:
```
Check if memtable flush is blocking
```

**Fixes**:
- Double buffering (two memtables)
- Larger memtable size
- Faster flush (parallel flush)
- SSD instead of HDD

### High Space Usage

**Symptom**: Space amp > 5x

**Diagnosis**:
```
Count tombstones and duplicate versions
```

**Fixes**:
- More aggressive compaction
- Drop tombstones earlier (risky!)
- Compression (LZ4/Snappy)

## Implementation Details

See [COMPONENT_GUIDE.md](../COMPONENT_GUIDE.md) for detailed explanations of:
- MemTable internals
- WAL format and recovery
- Bloom filter mathematics
- SSTable structure
- Compaction algorithms
- Performance tuning

## Comparison with Other Engines

| Feature | LSM-Tree | Hash Index | B-Tree |
|---------|----------|------------|--------|
| Write Speed | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Read Speed | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Range Scans | ‚úÖ Yes | ‚ùå No | ‚úÖ Yes |
| Space Efficiency | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Write Amp | 25x | 2x | 2-3x |
| Read Amp | 3-10x | 1x | 1-2x |
| Complexity | High | Low | Medium |

## Real-World Usage

LSM-Trees power many production systems:

- **LevelDB**: Google's embedded database
- **RocksDB**: Facebook's high-performance storage
- **Apache Cassandra**: Distributed database
- **Apache HBase**: Hadoop database
- **ScyllaDB**: High-performance NoSQL
- **TiKV**: Distributed transactional KV store

## References

### Academic Papers
- "The Log-Structured Merge-Tree (LSM-Tree)" - O'Neil et al., 1996
- "Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based KV Stores" - Harvard, 2018

### Implementation Guides
- LevelDB source code (C++)
- RocksDB documentation
- "Designing Data-Intensive Applications" by Martin Kleppmann (Chapter 3)

## License

MIT License - See [LICENSE](../LICENSE) file

## Contributing

Contributions welcome! Areas for improvement:
- Additional optimizations (see list above)
- Better benchmarking workloads
- Documentation improvements
- Bug fixes

---

**Built to understand LSM-Trees deeply** üöÄ
